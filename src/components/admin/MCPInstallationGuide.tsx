import { Card, CardContent } from "@/components/ui/card";
import { Alert, AlertDescription } from "@/components/ui/alert";
import { BookOpen, Terminal, Server, CheckCircle } from "lucide-react";
import { Separator } from "@/components/ui/separator";

export const MCPInstallationGuide = () => {
  return (
    <div className="space-y-4">
      <Alert>
        <BookOpen className="h-4 w-4" />
        <AlertDescription>
          Un serveur MCP vous permet d'utiliser des mod√®les IA personnalis√©s h√©berg√©s localement ou sur votre infrastructure.
        </AlertDescription>
      </Alert>

      <Card>
        <CardContent className="pt-6 space-y-4">
          <div className="space-y-3">
            <div className="flex items-center gap-2">
              <Server className="h-5 w-5 text-primary" />
              <h4 className="font-semibold">Option 1 : Ollama (Recommand√©)</h4>
            </div>
            <p className="text-sm text-muted-foreground">
              Ollama est la solution la plus simple pour ex√©cuter des mod√®les locaux.
            </p>
            <div className="bg-muted rounded-lg p-4 font-mono text-sm space-y-2">
              <div className="flex items-start gap-2">
                <Terminal className="h-4 w-4 mt-0.5 text-muted-foreground" />
                <div className="space-y-1 flex-1">
                  <p className="text-muted-foreground"># Installation (macOS/Linux)</p>
                  <p>curl -fsSL https://ollama.com/install.sh | sh</p>
                </div>
              </div>
              <Separator />
              <div className="flex items-start gap-2">
                <Terminal className="h-4 w-4 mt-0.5 text-muted-foreground" />
                <div className="space-y-1 flex-1">
                  <p className="text-muted-foreground"># D√©marrer le serveur</p>
                  <p>ollama serve</p>
                </div>
              </div>
              <Separator />
              <div className="flex items-start gap-2">
                <Terminal className="h-4 w-4 mt-0.5 text-muted-foreground" />
                <div className="space-y-1 flex-1">
                  <p className="text-muted-foreground"># T√©l√©charger un mod√®le</p>
                  <p>ollama pull llama2</p>
                </div>
              </div>
            </div>
            <div className="flex items-start gap-2 text-sm">
              <CheckCircle className="h-4 w-4 text-green-600 mt-0.5" />
              <p className="text-muted-foreground">
                URL √† utiliser : <code className="bg-muted px-1 py-0.5 rounded">http://localhost:11434</code>
              </p>
            </div>
          </div>

          <Separator />

          <div className="space-y-3">
            <div className="flex items-center gap-2">
              <Server className="h-5 w-5 text-primary" />
              <h4 className="font-semibold">Option 2 : LocalAI</h4>
            </div>
            <p className="text-sm text-muted-foreground">
              Alternative avec support de plus de mod√®les et API compatible OpenAI.
            </p>
            <div className="bg-muted rounded-lg p-4 font-mono text-sm space-y-2">
              <div className="flex items-start gap-2">
                <Terminal className="h-4 w-4 mt-0.5 text-muted-foreground" />
                <div className="space-y-1 flex-1">
                  <p className="text-muted-foreground"># Docker</p>
                  <p>docker run -p 8080:8080 localai/localai:latest</p>
                </div>
              </div>
            </div>
            <div className="flex items-start gap-2 text-sm">
              <CheckCircle className="h-4 w-4 text-green-600 mt-0.5" />
              <p className="text-muted-foreground">
                URL √† utiliser : <code className="bg-muted px-1 py-0.5 rounded">http://localhost:8080</code>
              </p>
            </div>
          </div>

          <Separator />

          <div className="space-y-3">
            <div className="flex items-center gap-2">
              <Server className="h-5 w-5 text-primary" />
              <h4 className="font-semibold">Option 3 : LM Studio</h4>
            </div>
            <p className="text-sm text-muted-foreground">
              Interface graphique pour g√©rer et ex√©cuter des mod√®les locaux.
            </p>
            <div className="space-y-2">
              <ol className="text-sm text-muted-foreground space-y-1 list-decimal list-inside">
                <li>T√©l√©chargez LM Studio depuis lmstudio.ai</li>
                <li>Installez et lancez l'application</li>
                <li>T√©l√©chargez un mod√®le depuis l'onglet "Discover"</li>
                <li>D√©marrez le serveur local depuis l'onglet "Local Server"</li>
              </ol>
              <div className="flex items-start gap-2 text-sm mt-2">
                <CheckCircle className="h-4 w-4 text-green-600 mt-0.5" />
                <p className="text-muted-foreground">
                  URL par d√©faut : <code className="bg-muted px-1 py-0.5 rounded">http://localhost:1234</code>
                </p>
              </div>
            </div>
          </div>

          <Separator />

          <div className="space-y-2">
            <h4 className="font-semibold text-sm">üîí S√©curit√© et r√©seau</h4>
            <ul className="text-sm text-muted-foreground space-y-1 list-disc list-inside">
              <li>Pour un serveur local : utilisez <code className="bg-muted px-1 py-0.5 rounded">http://localhost:PORT</code></li>
              <li>Pour un serveur sur le r√©seau : utilisez l'IP locale (ex: <code className="bg-muted px-1 py-0.5 rounded">http://192.168.1.100:8080</code>)</li>
              <li>Assurez-vous que le port est accessible depuis votre navigateur</li>
              <li>Pour un acc√®s externe s√©curis√©, configurez HTTPS et une cl√© API</li>
            </ul>
          </div>
        </CardContent>
      </Card>
    </div>
  );
};
